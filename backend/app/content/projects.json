[
  {
    "id": "qfi-capital",
    "title": "QFI Research Capital",
    "role": "Founding Engineer & Consultant",
    "description": "Algorithmic trading platform with mid‑frequency execution, real‑time analytics, risk management, and modular architecture. Built institutional‑grade data and execution pipelines with foundations for portfolio management and future AI‑driven signal generation.",
    "contribution": "Architected and led the initial build for four months (co‑founder → consultant).",
    "contributionBullets": [
      "Designed the event‑driven architecture and core trading domain model.",
      "Implemented real‑time WebSocket ingestion and Kafka producers for market data.",
      "Built Kafka consumers for ETL aggregation (multi‑interval OHLCV) into InfluxDB.",
      "Provisioned PostgreSQL for orders/positions/config; Redis for quotes/sessions.",
      "Integrated Kite Connect OAuth, token lifecycle, and throttled order execution.",
      "Exposed FastAPI REST endpoints for orders, positions, health, and data control.",
      "Set up observability (structured logs/metrics) and Dockerized local/staging envs."
    ],
    "tech": ["Python", "C++", "PostgreSQL", "InfluxDB", "Docker", "Kafka", "Redis", "Kite Connect"],
    "link": "https://qficapital.in",
    "embedSite": "https://qficapital.in",
    "status": "Live",
    "images": [
      "projects/qfi/qfi.png"
    ],
    "video": null,
    "videoPoster": null,
    "highlights": [
      "Mid‑frequency algorithmic trading with real‑time analytics and execution.",
      "Sub‑second Kafka streaming pipelines enabling timely signals and decisions.",
      "Institutional‑grade automation with risk controls and portfolio analytics.",
      "Improved reliability and latency through resilient services and observability.",
      "Backtesting and simulation groundwork for confident strategy validation.",
      "Multi‑asset capability (equities/derivatives) via a unified data plane.",
      "Co‑founder leadership: tech vision, mentoring, and early engineering hiring."
    ],
    "relatedProjects": ["trading-bot"],
    "relatedPosts": ["startup-journey"],
    "featured": true,
    "kind": "project"
  },
  {
    "id": "property-appraisal",
    "title": "Automated Property Appraisal Extraction",
    "role": "SDE Applied AI at Adaequare",
    "description": "Automated PDF report extraction that converts unstructured appraisal PDFs into structured JSON using a dual OCR + LLM pipeline.",
    "contribution": "Architected and built a modular dual‑pipeline (OCR + LLM) system end‑to‑end.",
    "contributionBullets": [
      "Implemented PDF splitting (Poppler) and image preprocessing (OpenCV).",
      "Integrated LayoutParser/Detectron2 for page layout and region extraction.",
      "Built OCR pipeline (Tesseract) with cleaning and deduplication.",
      "Persisted page‑level artifacts to JSONL for traceability and reprocessing.",
      "Developed LLM extraction with chunking, prompt templates, and merge/normalize.",
      "Exposed FastAPI routers to orchestrate OCR/LLM flows; added Streamlit dashboard.",
      "Centralized logging/config; standardized outputs for downstream analytics."
    ],
    "tech": ["FastAPI", "Python", "Poppler", "OpenCV", "Detectron2", "LayoutParser", "Tesseract", "GPT-4", "Streamlit", "JSONL", "PostgreSQL"],
    "status": "Production",
    "images": [
      "projects/property-appraisal/arch.jpg",
      "projects/property-appraisal/sample.jpg",
      "projects/property-appraisal/ss.jpg"
    ],
    "video": "https://www.loom.com/embed/0c90f480866644fc9677b4d7feaeb4af",
    "videoPoster": "projects/property-appraisal/videoframe.png",
    "highlights": [
      "Automates 10,000+ appraisal PDFs/year; ~300 hours/month saved; faster customer turnaround.",
      "Dual‑pipeline architecture (OCR + LLM) with clean separation for maintainability.",
      "High‑accuracy extraction via preprocessing, layout parsing, and normalization.",
      "API orchestration (FastAPI) and dashboard (Streamlit) for review and export.",
      "JSONL artifact trail for traceability, debugging, and repeatable reprocessing.",
      "Production‑ready design with centralized logging/config and extensible NLP pipeline."
    ],
    "relatedProjects": ["contract-extraction", "tax-roll-pipeline"],
    "relatedPosts": ["fastapi-practices"],
    "featured": true,
    "kind": "project"
  },
  {
    "id": "tax-roll-pipeline",
    "title": "Automated Tax Roll Processing Pipeline",
    "role": "SDE Applied AI at Adaequare",
    "description": "Enterprise data transformation system automating tax roll data matching across 3,000+ counties. A hybrid AI‑assisted two‑layer matching approach maps diverse county formats to standardized schemas, reducing analyst workload while preserving high accuracy.",
    "contribution": "Designed and delivered the hybrid AI‑assisted transformation platform end‑to‑end.",
    "contributionBullets": [
      "Two‑layer matching: Heuristics (token fuzzy, Jaccard, datatype scoring) + Gemini semantic matching on sampled values.",
      "Experimented with different matching algorithms and models to improve accuracy and recall.",
      "Built a feedback loop where analyst corrections improve future processing.",
      "Exposed FastAPI routers to orchestrate OCR/LLM flows as well as ETL pipeline operations.",
      "Target‑based architecture with multi‑source transformations and dependency tracking.",
      "Operator‑driven ETL engine: copy, trim, concat, static, arithmetic, and custom functions with full auditability.",
      "Preview mode for rapid QA; seconds vs. minutes for full ETL runs.",
      "React frontend for job management: uploads/merging, status tracking, target‑based review; localStorage + DB sync.",
      "Similarity score pipeline (0–1) integrated DB → API → UI as an analyst decision signal."
    ],
    "tech": ["Python 3.11", "FastAPI", "PostgreSQL (Azure)", "SQLAlchemy", "Pandas", "Google Gemini 2.5", "React 18", "TypeScript", "Vite", "React Query", "Node.js", "Docker", "OpenAPI"],
    "status": "Production",
    "images": [
      "projects/tax-roll-pipeline/ss1.png",
      "projects/tax-roll-pipeline/ss2.png",
      "projects/tax-roll-pipeline/ss3.png",
      "projects/tax-roll-pipeline/ss7.png",
      "projects/tax-roll-pipeline/ss9.png"
    ],
    "highlights": [
      "Significant reduction in analyst review time; higher throughput for operations.",
      "70%+ auto‑acceptance using AI-assisted matching without manual intervention.",
      "100% accuracy, 78% recall and an F1 score of 88%.",
      "Scalable architecture for 3,000+ counties.",
      "Production‑ready POC: OpenAPI docs, Docker‑ready, PII masking and sample limiting."
    ],
    "relatedProjects": ["contract-extraction", "property-appraisal"],
    "relatedPosts": ["etl-pipelines"],
    "featured": false,
    "kind": "project"
  },
  {
    "id": "contract-extraction",
    "title": "Contract Data Extraction Service",
    "role": "SDE Applied AI at Adaequare",
    "description": "AI-powered contract intelligence system that extracts structured metadata from various contract types (MSAs, NDAs, service agreements, etc.) into machine-readable JSON. Processes PDF and DOCX files using multimodal LLM extraction with template-based validation and quality scoring.",
    "contribution": "Architected and built the contract extraction system end-to-end, focusing on LLM integration, prompt engineering, state management, and validation workflows.",
    "contributionBullets": [
      "Designed extraction architecture supporting PDF (PyMuPDF) and DOCX (python-docx) formats with multimodal LLM processing.",
      "Engineered prompt templates with context management, state tracking, and memory optimization for efficient LLM calls.",
      "Implemented sophisticated validation system comparing extracted values against templates with expected answers, deviation detection, and quality scoring (0-100).",
      "Built FastAPI REST API with Pydantic models for request/response validation, async processing, and job status tracking.",
      "Developed field-level validation with match flags (same_as_template, similar_not_exact, different_from_template, flag_for_review, not_found) for automated review workflows.",
      "Created structured JSON schema (v2.0.0) with validation scoring, status indicators, and detailed notes for each extracted field.",
      "Optimized LLM context management and token usage through intelligent chunking and state preservation across extraction stages.",
      "Built CLI interface for single-file and batch processing with parallel execution support.",
      "Developed Streamlit UI for interactive contract review, validation, and extraction management.",
      "Deployed on GCP Cloud Run with Docker containers and Cloud SQL for persistence."
    ],
    "tech": ["Python 3.10+", "FastAPI", "Pydantic", "Google Gemini 3 Pro", "Google Cloud Vision API","PyMuPDF", "python-docx", "Streamlit", "Docker", "GCP Cloud Run", "Cloud SQL", "PostgreSQL"],
    "status": "Alpha (Beta Testing)",
    "images": [
      "projects/contract-extraction/msa_api.png"
    ],
    "video": null,
    "videoPoster": null,
    "highlights": [
      "Reduces contract review time from 3-4 hours to 15 minutes on average (some edge cases up to 1 hour); processes 5-10 contracts/month in v1.",
      "Enables early discrepancy detection by comparing extracted values against templates with deviation flags.",
      "Multimodal LLM approach (Gemini 2.5 Pro) handles both text and image-based contracts, replacing traditional OCR preprocessing.",
      "Template-based validation system with expected answers and deviation detection for automated quality assurance.",
      "Used by Legal, Sales, and Finance teams for contract metadata extraction and CRM automation.",
      "Production-ready architecture deployed on GCP with Docker containerization and Cloud SQL persistence."
    ],
    "relatedProjects": ["property-appraisal", "tax-roll-pipeline"],
    "relatedPosts": ["llm-patterns", "fastapi-practices"],
    "featured": true,
    "kind": "project"
  }
]


