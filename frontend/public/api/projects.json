[
  {
    "id": "qfi-capital",
    "title": "QFI Research Capital",
    "role": "Founding Engineer & Consultant",
    "description": "Algorithmic trading platform with mid‑frequency execution, real‑time analytics, risk management, and modular architecture. Built institutional‑grade data and execution pipelines with foundations for portfolio management and future AI‑driven signal generation.",
    "contribution": "Architected and led the initial build for four months (co‑founder → consultant).",
    "contributionBullets": [
      "Designed the event‑driven architecture and core trading domain model.",
      "Implemented real‑time WebSocket ingestion and Kafka producers for market data.",
      "Built Kafka consumers for ETL aggregation (multi‑interval OHLCV) into InfluxDB.",
      "Provisioned PostgreSQL for orders/positions/config; Redis for quotes/sessions.",
      "Integrated Kite Connect OAuth, token lifecycle, and throttled order execution.",
      "Exposed FastAPI REST endpoints for orders, positions, health, and data control.",
      "Set up observability (structured logs/metrics) and Dockerized local/staging envs."
    ],
    "tech": [
      "Python",
      "C++",
      "PostgreSQL",
      "InfluxDB",
      "Docker",
      "Kafka",
      "Redis",
      "Kite Connect"
    ],
    "link": "https://qficapital.in",
    "embedSite": "https://qficapital.in",
    "status": "Live",
    "images": [
      "projects/qfi/qfi.png"
    ],
    "video": null,
    "videoPoster": null,
    "highlights": [
      "Mid‑frequency algorithmic trading with real‑time analytics and execution.",
      "Sub‑second Kafka streaming pipelines enabling timely signals and decisions.",
      "Institutional‑grade automation with risk controls and portfolio analytics.",
      "Improved reliability and latency through resilient services and observability.",
      "Backtesting and simulation groundwork for confident strategy validation.",
      "Multi‑asset capability (equities/derivatives) via a unified data plane.",
      "Co‑founder leadership: tech vision, mentoring, and early engineering hiring."
    ],
    "relatedProjects": [
      "trading-and-backtesting-bot"
    ],
    "relatedPosts": [
      "startup-journey"
    ],
    "collaboration": {
      "seeking": false,
      "types": [],
      "contact": "/contact"
    },
    "service": {
      "available": false,
      "description": ""
    },
    "featured": true,
    "kind": "project"
  },
  {
    "id": "property-appraisal",
    "title": "Automated Property Appraisal Extraction",
    "role": "SDE Applied AI at Adaequare",
    "description": "Automated PDF report extraction that converts unstructured appraisal PDFs into structured JSON using a dual OCR + LLM pipeline.",
    "contribution": "Architected and built a modular dual‑pipeline (OCR + LLM) system end‑to‑end.",
    "contributionBullets": [
      "Implemented PDF splitting (Poppler) and image preprocessing (OpenCV).",
      "Integrated LayoutParser/Detectron2 for page layout and region extraction.",
      "Built OCR pipeline (Tesseract) with cleaning and deduplication.",
      "Persisted page‑level artifacts to JSONL for traceability and reprocessing.",
      "Developed LLM extraction with chunking, prompt templates, and merge/normalize.",
      "Exposed FastAPI routers to orchestrate OCR/LLM flows; added Streamlit dashboard.",
      "Centralized logging/config; standardized outputs for downstream analytics."
    ],
    "tech": [
      "FastAPI",
      "Python",
      "Poppler",
      "OpenCV",
      "Detectron2",
      "LayoutParser",
      "Tesseract",
      "GPT-4",
      "Streamlit",
      "JSONL",
      "PostgreSQL"
    ],
    "status": "Production",
    "images": [
      "projects/property-appraisal/arch.jpg",
      "projects/property-appraisal/sample.jpg",
      "projects/property-appraisal/ss.jpg"
    ],
    "video": "https://www.loom.com/embed/0c90f480866644fc9677b4d7feaeb4af",
    "videoPoster": "projects/property-appraisal/videoframe.png",
    "highlights": [
      "Automates 10,000+ appraisal PDFs/year; ~300 hours/month saved; faster customer turnaround.",
      "Dual‑pipeline architecture (OCR + LLM) with clean separation for maintainability.",
      "High‑accuracy extraction via preprocessing, layout parsing, and normalization.",
      "API orchestration (FastAPI) and dashboard (Streamlit) for review and export.",
      "JSONL artifact trail for traceability, debugging, and repeatable reprocessing.",
      "Production‑ready design with centralized logging/config and extensible NLP pipeline."
    ],
    "relatedProjects": [
      "contract-extraction",
      "tax-roll-pipeline"
    ],
    "relatedPosts": [
      "fastapi-practices"
    ],
    "collaboration": {
      "seeking": false,
      "types": [],
      "contact": "/contact"
    },
    "service": {
      "available": false,
      "description": ""
    },
    "featured": true,
    "kind": "project"
  },
  {
    "id": "tax-roll-pipeline",
    "title": "Automated Tax Roll Processing Pipeline",
    "role": "SDE Applied AI at Adaequare",
    "description": "Enterprise data transformation system automating tax roll data matching across 3,000+ counties. A hybrid AI‑assisted two‑layer matching approach maps diverse county formats to standardized schemas, reducing analyst workload while preserving high accuracy.",
    "contribution": "Designed and delivered the hybrid AI‑assisted transformation platform end‑to‑end.",
    "contributionBullets": [
      "Two‑layer matching: Heuristics (token fuzzy, Jaccard, datatype scoring) + Gemini semantic matching on sampled values.",
      "Experimented with different matching algorithms and models to improve accuracy and recall.",
      "Built a feedback loop where analyst corrections improve future processing.",
      "Exposed FastAPI routers to orchestrate OCR/LLM flows as well as ETL pipeline operations.",
      "Target‑based architecture with multi‑source transformations and dependency tracking.",
      "Operator‑driven ETL engine: copy, trim, concat, static, arithmetic, and custom functions with full auditability.",
      "Preview mode for rapid QA; seconds vs. minutes for full ETL runs.",
      "React frontend for job management: uploads/merging, status tracking, target‑based review; localStorage + DB sync.",
      "Similarity score pipeline (0–1) integrated DB → API → UI as an analyst decision signal."
    ],
    "tech": [
      "Python 3.11",
      "FastAPI",
      "PostgreSQL (Azure)",
      "SQLAlchemy",
      "Pandas",
      "Google Gemini 2.5",
      "React 18",
      "TypeScript",
      "Vite",
      "React Query",
      "Node.js",
      "Docker",
      "OpenAPI"
    ],
    "status": "Production",
    "images": [
      "projects/tax-roll-pipeline/ss1.png",
      "projects/tax-roll-pipeline/ss2.png",
      "projects/tax-roll-pipeline/ss3.png",
      "projects/tax-roll-pipeline/ss7.png",
      "projects/tax-roll-pipeline/ss9.png"
    ],
    "highlights": [
      "Significant reduction in analyst review time; higher throughput for operations.",
      "70%+ auto‑acceptance using AI-assisted matching without manual intervention.",
      "100% accuracy, 78% recall and an F1 score of 88%.",
      "Scalable architecture for 3,000+ counties.",
      "Production‑ready POC: OpenAPI docs, Docker‑ready, PII masking and sample limiting."
    ],
    "relatedProjects": [
      "contract-extraction",
      "property-appraisal"
    ],
    "relatedPosts": [
      "etl-pipelines"
    ],
    "collaboration": {
      "seeking": false,
      "types": [],
      "contact": "/contact"
    },
    "service": {
      "available": false,
      "description": ""
    },
    "featured": false,
    "kind": "project"
  },
  {
    "id": "contract-extraction",
    "title": "Contract Data Extraction Service",
    "role": "SDE Applied AI at Adaequare",
    "description": "AI-powered contract intelligence system that extracts structured metadata from various contract types (MSAs, NDAs, service agreements, etc.) into machine-readable JSON. Processes PDF and DOCX files using multimodal LLM extraction with template-based validation and quality scoring.",
    "contribution": "Architected and built the contract extraction system end-to-end, focusing on LLM integration, prompt engineering, state management, and validation workflows.",
    "contributionBullets": [
      "Designed extraction architecture supporting PDF (PyMuPDF) and DOCX (python-docx) formats with multimodal LLM processing.",
      "Engineered prompt templates with context management, state tracking, and memory optimization for efficient LLM calls.",
      "Implemented sophisticated validation system comparing extracted values against templates with expected answers, deviation detection, and quality scoring (0-100).",
      "Built FastAPI REST API with Pydantic models for request/response validation, async processing, and job status tracking.",
      "Developed field-level validation with match flags (same_as_template, similar_not_exact, different_from_template, flag_for_review, not_found) for automated review workflows.",
      "Created structured JSON schema (v2.0.0) with validation scoring, status indicators, and detailed notes for each extracted field.",
      "Optimized LLM context management and token usage through intelligent chunking and state preservation across extraction stages.",
      "Built CLI interface for single-file and batch processing with parallel execution support.",
      "Developed Streamlit UI for interactive contract review, validation, and extraction management.",
      "Deployed on GCP Cloud Run with Docker containers and Cloud SQL for persistence."
    ],
    "tech": [
      "Python 3.10+",
      "FastAPI",
      "Pydantic",
      "Google Gemini 3 Pro",
      "Google Cloud Vision API",
      "PyMuPDF",
      "python-docx",
      "Streamlit",
      "Docker",
      "GCP Cloud Run",
      "Cloud SQL",
      "PostgreSQL"
    ],
    "status": "Alpha (Beta Testing)",
    "images": [
      "projects/contract-extraction/arch.png",
      "projects/contract-extraction/msa_api.png"
    ],
    "video": null,
    "videoPoster": null,
    "highlights": [
      "Reduces contract review time from 3-4 hours to 15 minutes on average (some edge cases up to 1 hour); processes 5-10 contracts/month in v1.",
      "Enables early discrepancy detection by comparing extracted values against templates with deviation flags.",
      "Multimodal LLM approach (Gemini 3 Pro/Flash) handles both text and image-based contracts, replacing traditional OCR preprocessing.",
      "Template-based validation system with expected answers and deviation detection for automated quality assurance.",
      "Used by Legal, Sales, and Finance teams for contract metadata extraction and CRM automation.",
      "Production-ready architecture deployed on GCP with Docker containerization and Cloud SQL persistence."
    ],
    "relatedProjects": [
      "property-appraisal",
      "tax-roll-pipeline"
    ],
    "relatedPosts": [
      "llm-patterns",
      "fastapi-practices"
    ],
    "collaboration": {
      "seeking": false,
      "types": [],
      "contact": "/contact"
    },
    "service": {
      "available": false,
      "description": ""
    },
    "featured": true,
    "kind": "project"
  },
  {
    "id": "q-rate",
    "title": "Q-Rate",
    "description": "Loyalty and verification system for cafes that eliminates social friction in review collection and reduces fraud risk. Built based on existing loyalty programs, with WhatsApp integration, digital stamp cards, and point-based redemption. Currently in active development with a clear roadmap from verified reviews (V0) to a unified discovery platform (V2).",
    "tech": [
      "Python",
      "FastAPI",
      "PostgreSQL",
      "React",
      "Vite",
      "TypeScript",
      "PWA",
      "WhatsApp Business API",
      "Docker"
    ],
    "details": "I'm building Q-Rate as a loyalty platform for Hyderabad's cafe market, inspired by existing loyalty programs but designed to solve specific pain points I've observed. The system includes customer and staff PWAs, an admin portal for cafe owners, and WhatsApp Business API integration for seamless customer engagement. The goal is to make it easy for staff to enroll customers and manage loyalty points while providing analytics and fraud prevention. This is something I'm actively working on and trying to build into a real product.",
    "highlights": [
      "WhatsApp Business API integration for zero-friction customer engagement",
      "Dual PWA architecture: Separate customer and staff applications",
      "Digital stamp cards and point-based redemption system",
      "Admin portal with analytics, loyalty ratio configuration, and staff performance tracking",
      "Multi-phase roadmap: From verified reviews to unified discovery platform",
      "Built based on existing loyalty programs with improvements for the cafe market"
    ],
    "link": "https://github.com/keerthanvenkata/q-rate",
    "collaboration": {
      "seeking": true,
      "types": [
        "Development",
        "Technical Architecture",
        "Business Strategy",
        "Product Management",
        "Sales & Marketing"
      ],
      "contact": "/contact"
    },
    "service": {
      "available": false,
      "description": ""
    },
    "relatedPosts": [
      "startup-journey"
    ],
    "images": [],
    "featured": false,
    "kind": "experimental"
  },
  {
    "id": "document-data-extraction",
    "title": "Document Data Extraction",
    "description": "Generalized document extraction framework built as an attempt to abstract and generalize the property appraisal extraction project. Inspired by that work, this framework features modular architecture with separate OCR and LLM pipelines for document processing.",
    "tech": [
      "Python",
      "FastAPI",
      "Streamlit",
      "Tesseract",
      "LayoutParser",
      "Detectron2",
      "Poppler",
      "LLM"
    ],
    "details": "After building the property appraisal extraction system, I wanted to see if I could generalize the approach for other document types. This project was my attempt to abstract and generalize that work into a reusable framework. The initial idea was to make it easily customizable for different applications, though I didn't end up focusing heavily on that customization aspect. Instead, I focused more on the core extraction pipelines - separate OCR and LLM modules that can be swapped or extended. It's a learning project that helped me understand how to structure document processing systems.",
    "highlights": [
      "Built to abstract and generalize the property appraisal extraction project",
      "Modular OCR and LLM pipelines for flexible document processing",
      "FastAPI REST endpoints for programmatic access",
      "Streamlit dashboard for interactive data review and export",
      "Inspired by real-world document extraction needs"
    ],
    "link": "https://github.com/keerthanvenkata/document_data_extraction",
    "collaboration": {
      "seeking": false,
      "types": [],
      "contact": "/contact"
    },
    "service": {
      "available": true,
      "description": "This framework can be customized for specific document types and extraction requirements. Available for custom development projects."
    },
    "relatedProjects": [
      "property-appraisal"
    ],
    "images": [],
    "featured": false,
    "kind": "experimental"
  },
  {
    "id": "cli-tool",
    "title": "CLI Tool for Data Cleaning",
    "description": "Utility for quick CSV transformations.",
    "tech": [
      "Python",
      "Click"
    ],
    "details": "A utility tool I built for prepping tax roll data. It handles repetitive CSV cleaning tasks that come up during data preparation workflows, including transformations, validations, and batch processing operations needed before data can be fed into larger processing pipelines.",
    "highlights": [
      "Quick transformations",
      "Batch processing"
    ],
    "collaboration": {
      "seeking": false,
      "types": [],
      "contact": "/contact"
    },
    "service": {
      "available": false,
      "description": ""
    },
    "relatedProjects": [
      "tax-roll-pipeline"
    ],
    "images": [],
    "featured": false,
    "kind": "experimental"
  },
  {
    "id": "trading-and-backtesting-bot",
    "title": "Trading and Backtesting Bot",
    "description": "Algorithmic trading bot with backtesting capabilities for strategy validation and execution.",
    "tech": [
      "Python",
      "PostgreSQL",
      "InfluxDB",
      "Docker",
      "Kafka",
      "Kite Connect"
    ],
    "details": "I built this trading bot primarily to verify quantitative assumptions and get a better understanding of market dynamics. It's not meant for live trading or production use, but rather as a learning tool to test strategies, understand backtesting methodologies, and explore how different market conditions affect trading algorithms. The bot includes real-time data processing, backtesting framework, and execution capabilities to validate theoretical approaches with real market data.",
    "highlights": [
      "Real-time market data processing",
      "Backtesting framework",
      "Strategy execution"
    ],
    "collaboration": {
      "seeking": false,
      "types": [],
      "contact": "/contact"
    },
    "service": {
      "available": false,
      "description": ""
    },
    "relatedProjects": [
      "qfi-capital"
    ],
    "images": [],
    "featured": false,
    "kind": "experimental"
  }
]