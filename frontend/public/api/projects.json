[
  {
    "id": "qfi-capital",
    "title": "QFI Research Capital",
    "role": "Founding Engineer & Consultant",
    "description": "Algorithmic trading platform with mid‑frequency execution, real‑time analytics, risk management, and modular architecture. Built institutional‑grade data and execution pipelines with foundations for portfolio management and future AI‑driven signal generation.",
    "contribution": "Architected and led the initial build for four months (co‑founder → consultant).",
    "contributionBullets": [
      "Designed the event‑driven architecture and core trading domain model.",
      "Implemented real‑time WebSocket ingestion and Kafka producers for market data.",
      "Built Kafka consumers for ETL aggregation (multi‑interval OHLCV) into InfluxDB.",
      "Provisioned PostgreSQL for orders/positions/config; Redis for quotes/sessions.",
      "Integrated Kite Connect OAuth, token lifecycle, and throttled order execution.",
      "Exposed FastAPI REST endpoints for orders, positions, health, and data control.",
      "Set up observability (structured logs/metrics) and Dockerized local/staging envs."
    ],
    "tech": [
      "Python",
      "C++",
      "PostgreSQL",
      "InfluxDB",
      "Docker",
      "Kafka",
      "Redis",
      "Kite Connect"
    ],
    "link": "https://qficapital.in",
    "embedSite": "https://qficapital.in",
    "status": "Live",
    "images": [
      "projects/qfi/qfi.png"
    ],
    "video": null,
    "videoPoster": null,
    "highlights": [
      "Mid‑frequency algorithmic trading with real‑time analytics and execution.",
      "Sub‑second Kafka streaming pipelines enabling timely signals and decisions.",
      "Institutional‑grade automation with risk controls and portfolio analytics.",
      "Improved reliability and latency through resilient services and observability.",
      "Backtesting and simulation groundwork for confident strategy validation.",
      "Multi‑asset capability (equities/derivatives) via a unified data plane.",
      "Co‑founder leadership: tech vision, mentoring, and early engineering hiring."
    ],
    "relatedProjects": [
      "trading-and-backtesting-bot"
    ],
    "relatedPosts": [
      "startup-journey"
    ],
    "collaboration": {
      "seeking": false,
      "types": [],
      "contact": "/contact"
    },
    "service": {
      "available": false,
      "description": ""
    },
    "featured": true,
    "kind": "project"
  },
  {
    "id": "property-appraisal",
    "title": "Automated Property Appraisal Extraction",
    "role": "SDE Applied AI at Adaequare",
    "description": "Automated PDF report extraction that converts unstructured appraisal PDFs into structured JSON using a dual OCR + LLM pipeline.",
    "contribution": "Architected and built a modular dual‑pipeline (OCR + LLM) system end‑to‑end.",
    "contributionBullets": [
      "Implemented PDF splitting (Poppler) and image preprocessing (OpenCV).",
      "Integrated LayoutParser/Detectron2 for page layout and region extraction.",
      "Built OCR pipeline (Tesseract) with cleaning and deduplication.",
      "Persisted page‑level artifacts to JSONL for traceability and reprocessing.",
      "Developed LLM extraction with chunking, prompt templates, and merge/normalize.",
      "Exposed FastAPI routers to orchestrate OCR/LLM flows; added Streamlit dashboard.",
      "Centralized logging/config; standardized outputs for downstream analytics."
    ],
    "tech": [
      "FastAPI",
      "Python",
      "Poppler",
      "OpenCV",
      "Detectron2",
      "LayoutParser",
      "Tesseract",
      "GPT-4",
      "Streamlit",
      "JSONL",
      "PostgreSQL"
    ],
    "status": "Production",
    "images": [
      "projects/property-appraisal/arch.jpg",
      "projects/property-appraisal/sample.jpg",
      "projects/property-appraisal/ss.jpg"
    ],
    "video": "https://www.loom.com/embed/0c90f480866644fc9677b4d7feaeb4af",
    "videoPoster": "projects/property-appraisal/videoframe.png",
    "highlights": [
      "Automates 10,000+ appraisal PDFs/year; ~300 hours/month saved; faster customer turnaround.",
      "Dual‑pipeline architecture (OCR + LLM) with clean separation for maintainability.",
      "High‑accuracy extraction via preprocessing, layout parsing, and normalization.",
      "API orchestration (FastAPI) and dashboard (Streamlit) for review and export.",
      "JSONL artifact trail for traceability, debugging, and repeatable reprocessing.",
      "Production‑ready design with centralized logging/config and extensible NLP pipeline."
    ],
    "relatedProjects": [
      "contract-extraction",
      "tax-roll-pipeline"
    ],
    "relatedPosts": [
      "fastapi-practices"
    ],
    "collaboration": {
      "seeking": false,
      "types": [],
      "contact": "/contact"
    },
    "service": {
      "available": false,
      "description": ""
    },
    "featured": true,
    "kind": "project"
  },
  {
    "id": "tax-roll-pipeline",
    "title": "Automated Tax Roll Processing Pipeline",
    "role": "SDE Applied AI at Adaequare",
    "description": "Enterprise data transformation system automating tax roll data matching across 3,000+ counties. A hybrid AI‑assisted two‑layer matching approach maps diverse county formats to standardized schemas, reducing analyst workload while preserving high accuracy.",
    "contribution": "Designed and delivered the hybrid AI‑assisted transformation platform end‑to‑end.",
    "contributionBullets": [
      "Two‑layer matching: Heuristics (token fuzzy, Jaccard, datatype scoring) + Gemini semantic matching on sampled values.",
      "Experimented with different matching algorithms and models to improve accuracy and recall.",
      "Built a feedback loop where analyst corrections improve future processing.",
      "Exposed FastAPI routers to orchestrate OCR/LLM flows as well as ETL pipeline operations.",
      "Target‑based architecture with multi‑source transformations and dependency tracking.",
      "Operator‑driven ETL engine: copy, trim, concat, static, arithmetic, and custom functions with full auditability.",
      "Preview mode for rapid QA; seconds vs. minutes for full ETL runs.",
      "React frontend for job management: uploads/merging, status tracking, target‑based review; localStorage + DB sync.",
      "Similarity score pipeline (0–1) integrated DB → API → UI as an analyst decision signal."
    ],
    "tech": [
      "Python 3.11",
      "FastAPI",
      "PostgreSQL (Azure)",
      "SQLAlchemy",
      "Pandas",
      "Google Gemini 2.5",
      "React 18",
      "TypeScript",
      "Vite",
      "React Query",
      "Node.js",
      "Docker",
      "OpenAPI"
    ],
    "status": "Demo",
    "images": [
      "projects/tax-roll-pipeline/ss1.png",
      "projects/tax-roll-pipeline/ss2.png",
      "projects/tax-roll-pipeline/ss3.png",
      "projects/tax-roll-pipeline/ss7.png",
      "projects/tax-roll-pipeline/ss9.png"
    ],
    "highlights": [
      "Significant reduction in analyst review time; higher throughput for operations.",
      "70%+ auto‑acceptance using AI-assisted matching without manual intervention.",
      "100% accuracy, 78% recall and an F1 score of 88%.",
      "Scalable architecture for 3,000+ counties.",
      "Production‑ready POC: OpenAPI docs, Docker‑ready, PII masking and sample limiting."
    ],
    "relatedProjects": [
      "contract-extraction",
      "property-appraisal"
    ],
    "relatedPosts": [
      "etl-pipelines"
    ],
    "collaboration": {
      "seeking": false,
      "types": [],
      "contact": "/contact"
    },
    "service": {
      "available": false,
      "description": ""
    },
    "featured": false,
    "kind": "project"
  },
  {
    "id": "contract-extraction",
    "title": "Contract Data Extraction Service",
    "role": "SDE Applied AI at Adaequare",
    "description": "AI-powered contract intelligence system that extracts structured metadata from various contract types (MSAs, NDAs, service agreements, etc.) into machine-readable JSON. Processes PDF and DOCX using multimodal LLM extraction with template-based validation and quality scoring. This production system became the foundation for domain-specific extraction services—later adapted into a streamlined event flyer extraction pipeline.",
    "contribution": "Architected and built the contract extraction system end-to-end, establishing the patterns and validation workflows that became the template for extraction services across domains.",
    "contributionBullets": [
      "Designed extraction architecture supporting PDF (PyMuPDF) and DOCX (python-docx) with multimodal LLM processing.",
      "Engineered prompt templates with context management, state tracking, and memory optimization for efficient LLM calls.",
      "Implemented validation comparing extracted values to templates with expected answers, deviation detection, and quality scoring (0-100).",
      "Built FastAPI REST API with Pydantic models for request/response validation, async processing, and job status tracking.",
      "Developed field-level validation with match flags (same_as_template, similar_not_exact, different_from_template, flag_for_review, not_found) for automated review.",
      "Created structured JSON schema (v2.0.0) with validation scoring, status indicators, and detailed notes per field.",
      "Optimized LLM context and token usage via chunking and state preservation across extraction stages.",
      "Built CLI for single-file and batch processing with parallel execution.",
      "Developed Streamlit UI for interactive contract review, validation, and extraction management.",
      "Deployed on GCP Cloud Run with Docker and Cloud SQL for persistence.",
      "Established reusable extraction framework that enabled rapid adaptation for event flyer processing (event-flyer-extractor)."
    ],
    "tech": [
      "Python 3.10+",
      "FastAPI",
      "Pydantic",
      "Google Gemini 3 Pro",
      "Google Cloud Vision API",
      "PyMuPDF",
      "python-docx",
      "Streamlit",
      "Docker",
      "GCP Cloud Run",
      "Cloud SQL",
      "PostgreSQL"
    ],
    "link": "https://github.com/keerthanvenkata/msa_extractor",
    "status": "Production",
    "images": [
      "projects/contract-extraction/arch.png",
      "projects/contract-extraction/msa_api.png"
    ],
    "video": null,
    "videoPoster": null,
    "highlights": [
      "Reduces contract review time from 3-4 hours to 15 minutes on average (some edge cases up to 1 hour); processes 5-10 contracts/month in v1.",
      "Enables early discrepancy detection by comparing extracted values against templates with deviation flags.",
      "Multimodal LLM approach (Gemini 3 Pro/Flash) handles both text and image-based contracts, replacing traditional OCR preprocessing.",
      "Template-based validation system with expected answers and deviation detection for automated quality assurance.",
      "Used by Legal, Sales, and Finance teams for contract metadata extraction and CRM automation.",
      "Production-ready architecture deployed on GCP with Docker containerization and Cloud SQL persistence.",
      "Reusable foundation: patterns adapted for domain-specific services (e.g. event flyer extractor)."
    ],
    "relatedProjects": [
      "property-appraisal",
      "tax-roll-pipeline",
      "event-flyer-extractor"
    ],
    "relatedPosts": [
      "llm-patterns",
      "fastapi-practices",
      "contract-intelligence-takeaways"
    ],
    "collaboration": {
      "seeking": false,
      "types": [],
      "contact": "/contact"
    },
    "service": {
      "available": false,
      "description": ""
    },
    "featured": true,
    "kind": "project"
  },
  {
    "id": "semantic-topic-mapper",
    "title": "Semantic Topic Mapper",
    "role": "Author & Maintainer",
    "description": "Python tool that extracts topic hierarchies, cross-references, and entity relationships from complex documents. Enables structured navigation and semantic understanding of document content through intelligent topic extraction and relationship mapping.",
    "contribution": "Architected and built an end-to-end semantic document analysis system for hierarchical topic extraction.",
    "contributionBullets": [
      "Designed semantic analysis pipeline for extracting topic hierarchies from unstructured documents.",
      "Implemented entity relationship extraction and cross-reference mapping.",
      "Built hierarchical topic organization system for structured navigation.",
      "Developed document parsing and semantic understanding components.",
      "Created APIs for topic extraction, relationship queries, and navigation services."
    ],
    "tech": [
      "Python",
      "FastAPI",
      "Google Gemini",
      "Markdown",
      "Pydantic"
    ],
    "link": "https://github.com/keerthanvenkata/semantic-topic-mapper",
    "status": "Active",
    "images": [
      "projects/semantic-topic-mapper/infographic.png",
      "projects/semantic-topic-mapper/arch_diagram.png"
    ],
    "video": null,
    "videoPoster": null,
    "highlights": [
      "Extracts meaningful topic hierarchies from complex, unstructured documents.",
      "Maps entity relationships and cross-references for comprehensive semantic understanding.",
      "Enables structured navigation through document hierarchies.",
      "Supports scalable document analysis workflows.",
      "MIT licensed for open collaboration and extension."
    ],
    "relatedProjects": [
      "document-data-extraction",
      "contract-extraction"
    ],
    "relatedPosts": [
      "contract-intelligence-takeaways"
    ],
    "collaboration": {
      "seeking": false,
      "types": [],
      "contact": "/contact"
    },
    "service": {
      "available": false,
      "description": ""
    },
    "featured": false,
    "kind": "project"
  },
  {
    "id": "event-flyer-extractor",
    "title": "Event Flyer Extraction System",
    "role": "AI & Technical Consultant (BookMyStall.in)",
    "description": "Production-ready AI system that extracts structured event metadata from flyer images using Gemini Vision. Processes JPG/PNG to extract 11 event fields (name, date, venue, organizer, etc.) with validation and human-in-the-loop review. Built as a streamlined, domain-specific adaptation of the Contract Data Extraction Service.",
    "contribution": "Architected and deployed a production-optimized extraction system that adapts proven patterns from enterprise contract extraction into a cost-effective event intelligence platform.",
    "contributionBullets": [
      "Adapted template-based validation from contract extraction for event-specific metadata (11 core fields).",
      "Implemented Gemini Vision-based extraction (Flash for cost efficiency while keeping accuracy on image content).",
      "Built two-tier validation: Python datatype checks plus LLM-based quality assessment with 13 confidence flags.",
      "Designed SQLite-based job tracking for simplicity and zero-ops (inspired by contract extraction DB patterns).",
      "Created external prompt management so non-technical users can iterate on extraction rules without code changes.",
      "Developed Streamlit UI for human review and approval workflows, aligned with contract extraction review interface.",
      "Integrated Google Drive polling (cron) for automated batch ingestion from client sources.",
      "Built JSONL export for downstream integration (Google Sheets, booking platforms).",
      "Centralized logging and configuration patterns from contract extraction.",
      "Deployed with minimal infrastructure: local scripts plus optional FastAPI, scaled for ~1,000 images/month."
    ],
    "tech": [
      "Python 3.11",
      "Gemini 2.0 Flash",
      "FastAPI",
      "Streamlit",
      "SQLite",
      "Google Drive API",
      "JSONL",
      "Docker"
    ],
    "link": "https://github.com/keerthanvenkata/event-flyer-extractor",
    "status": "Production",
    "images": [
      "projects/event-flyer-extractor/infographic-1.png",
      "projects/event-flyer-extractor/infographic-2.png",
      "projects/event-flyer-extractor/review-ui.png",
      "projects/event-flyer-extractor/uml.svg"
    ],
    "video": null,
    "videoPoster": null,
    "highlights": [
      "Processes ~1,000 event flyer images/month with 90%+ auto-approval via intelligent validation.",
      "Reduces manual data entry from ~5 mins/event to under 2 mins (review-only workflow).",
      "Domain-specific adaptation of enterprise contract extraction patterns; proves architectural reusability.",
      "13-flag validation prioritizes edge cases for human review.",
      "Production-ready with Google Drive integration, JSONL export, and Streamlit dashboard.",
      "Pragmatic design: simple, reliable, easy to iterate—minimal infrastructure can match enterprise effectiveness."
    ],
    "relatedProjects": [
      "contract-extraction"
    ],
    "relatedPosts": [
      "llm-patterns",
      "contract-intelligence-takeaways"
    ],
    "collaboration": {
      "seeking": false,
      "types": [],
      "contact": "/contact"
    },
    "service": {
      "available": false,
      "description": ""
    },
    "featured": true,
    "kind": "project"
  },
  {
    "id": "q-rate",
    "title": "Q-Rate",
    "description": "Loyalty and verification system for cafes that eliminates social friction in review collection and reduces fraud risk. Built based on existing loyalty programs, with WhatsApp integration, digital stamp cards, and point-based redemption. Currently in active development with a clear roadmap from verified reviews (V0) to a unified discovery platform (V2).",
    "tech": [
      "Python",
      "FastAPI",
      "PostgreSQL",
      "React",
      "Vite",
      "TypeScript",
      "PWA",
      "WhatsApp Business API",
      "Docker"
    ],
    "details": "I'm building Q-Rate as a loyalty platform for Hyderabad's cafe market, inspired by existing loyalty programs but designed to solve specific pain points I've observed. The system includes customer and staff PWAs, an admin portal for cafe owners, and WhatsApp Business API integration for seamless customer engagement. The goal is to make it easy for staff to enroll customers and manage loyalty points while providing analytics and fraud prevention. This is something I'm actively working on and trying to build into a real product.",
    "highlights": [
      "WhatsApp Business API integration for zero-friction customer engagement",
      "Dual PWA architecture: Separate customer and staff applications",
      "Digital stamp cards and point-based redemption system",
      "Admin portal with analytics, loyalty ratio configuration, and staff performance tracking",
      "Multi-phase roadmap: From verified reviews to unified discovery platform",
      "Built based on existing loyalty programs with improvements for the cafe market"
    ],
    "link": "https://github.com/keerthanvenkata/q-rate",
    "collaboration": {
      "seeking": true,
      "types": [
        "Development",
        "Technical Architecture",
        "Business Strategy",
        "Product Management",
        "Sales & Marketing"
      ],
      "contact": "/contact"
    },
    "service": {
      "available": false,
      "description": ""
    },
    "relatedPosts": [
      "startup-journey"
    ],
    "images": [],
    "featured": false,
    "kind": "experimental"
  },
  {
    "id": "document-data-extraction",
    "title": "Document Data Extraction",
    "description": "Generalized document extraction framework built as an attempt to abstract and generalize the property appraisal extraction project. Inspired by that work, this framework features modular architecture with separate OCR and LLM pipelines for document processing.",
    "tech": [
      "Python",
      "FastAPI",
      "Streamlit",
      "Tesseract",
      "LayoutParser",
      "Detectron2",
      "Poppler",
      "LLM"
    ],
    "details": "After building the property appraisal extraction system, I wanted to see if I could generalize the approach for other document types. This project was my attempt to abstract and generalize that work into a reusable framework. The initial idea was to make it easily customizable for different applications, though I didn't end up focusing heavily on that customization aspect. Instead, I focused more on the core extraction pipelines - separate OCR and LLM modules that can be swapped or extended. It's a learning project that helped me understand how to structure document processing systems.",
    "highlights": [
      "Built to abstract and generalize the property appraisal extraction project",
      "Modular OCR and LLM pipelines for flexible document processing",
      "FastAPI REST endpoints for programmatic access",
      "Streamlit dashboard for interactive data review and export",
      "Inspired by real-world document extraction needs"
    ],
    "link": "https://github.com/keerthanvenkata/document_data_extraction",
    "collaboration": {
      "seeking": false,
      "types": [],
      "contact": "/contact"
    },
    "service": {
      "available": true,
      "description": "This framework can be customized for specific document types and extraction requirements. Available for custom development projects."
    },
    "relatedProjects": [
      "property-appraisal"
    ],
    "images": [],
    "featured": false,
    "kind": "experimental"
  },
  {
    "id": "trading-and-backtesting-bot",
    "title": "Trading and Backtesting Bot",
    "description": "Internal experimental tool used while building QFI Research Capital. Backtesting and strategy-validation sandbox aligned with the same data and execution stack (Kafka, InfluxDB, Kite Connect) that informed the main platform. No public link—internal R&D only.",
    "tech": [
      "Python",
      "PostgreSQL",
      "InfluxDB",
      "Docker",
      "Kafka",
      "Kite Connect"
    ],
    "details": "While working on QFI we built this bot as an internal sandbox to verify quantitative assumptions, test strategies, and validate backtesting methodologies before and alongside the main platform. It shares the same event-driven ideas and stack: real-time market data via Kafka, OHLCV aggregation into InfluxDB, and execution via Kite Connect. The goal was to run experiments and understand market dynamics without affecting production, and to inform design decisions for the institutional platform. That’s why it stays unlinked—it was internal tooling, not a shipped product.",
    "highlights": [
      "Internal R&D tool used during QFI Research Capital; no public deployment or link",
      "Backtesting and simulation for strategy validation, aligned with QFI’s production stack",
      "Same data plane: Kafka, InfluxDB, PostgreSQL, Kite Connect",
      "Real-time market data processing and execution sandbox for quantitative experiments"
    ],
    "collaboration": {
      "seeking": false,
      "types": [],
      "contact": "/contact"
    },
    "service": {
      "available": false,
      "description": ""
    },
    "relatedProjects": [
      "qfi-capital"
    ],
    "images": [],
    "featured": false,
    "kind": "experimental"
  }
]