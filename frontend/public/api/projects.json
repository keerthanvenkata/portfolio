[
  {
    "id": "qfi-capital",
    "title": "QFI Research Capital",
    "role": "Founding Engineer & Consultant",
    "description": "Algorithmic trading platform with mid‑frequency execution, real‑time analytics, risk management, and modular architecture. Built institutional‑grade data and execution pipelines with foundations for portfolio management and future AI‑driven signal generation.",
    "contribution": "Architected and led the initial build for four months (co‑founder → consultant).",
    "contributionBullets": [
      "Designed the event‑driven architecture and core trading domain model.",
      "Implemented real‑time WebSocket ingestion and Kafka producers for market data.",
      "Built Kafka consumers for ETL aggregation (multi‑interval OHLCV) into InfluxDB.",
      "Provisioned PostgreSQL for orders/positions/config; Redis for quotes/sessions.",
      "Integrated Kite Connect OAuth, token lifecycle, and throttled order execution.",
      "Exposed FastAPI REST endpoints for orders, positions, health, and data control.",
      "Set up observability (structured logs/metrics) and Dockerized local/staging envs."
    ],
    "tech": [
      "Python",
      "C++",
      "PostgreSQL",
      "InfluxDB",
      "Docker",
      "Kafka",
      "Redis",
      "Kite Connect"
    ],
    "link": "https://qficapital.in",
    "embedSite": "https://qficapital.in",
    "status": "Live",
    "images": [
      "projects/qfi/qfi.png"
    ],
    "video": null,
    "videoPoster": null,
    "highlights": [
      "Mid‑frequency algorithmic trading with real‑time analytics and execution.",
      "Sub‑second Kafka streaming pipelines enabling timely signals and decisions.",
      "Institutional‑grade automation with risk controls and portfolio analytics.",
      "Improved reliability and latency through resilient services and observability.",
      "Backtesting and simulation groundwork for confident strategy validation.",
      "Multi‑asset capability (equities/derivatives) via a unified data plane.",
      "Co‑founder leadership: tech vision, mentoring, and early engineering hiring."
    ],
    "relatedProjects": [
      "trading-bot"
    ],
    "relatedPosts": [
      "startup-journey"
    ],
    "featured": true,
    "kind": "project"
  },
  {
    "id": "property-appraisal",
    "title": "Automated Property Appraisal Extraction",
    "role": "SDE Applied AI at Adaequare",
    "description": "Automated PDF report extraction that converts unstructured appraisal PDFs into structured JSON using a dual OCR + LLM pipeline.",
    "contribution": "Architected and built a modular dual‑pipeline (OCR + LLM) system end‑to‑end.",
    "contributionBullets": [
      "Implemented PDF splitting (Poppler) and image preprocessing (OpenCV).",
      "Integrated LayoutParser/Detectron2 for page layout and region extraction.",
      "Built OCR pipeline (Tesseract) with cleaning and deduplication.",
      "Persisted page‑level artifacts to JSONL for traceability and reprocessing.",
      "Developed LLM extraction with chunking, prompt templates, and merge/normalize.",
      "Exposed FastAPI routers to orchestrate OCR/LLM flows; added Streamlit dashboard.",
      "Centralized logging/config; standardized outputs for downstream analytics."
    ],
    "tech": [
      "FastAPI",
      "Python",
      "Poppler",
      "OpenCV",
      "Detectron2",
      "LayoutParser",
      "Tesseract",
      "GPT-4",
      "Streamlit",
      "JSONL",
      "PostgreSQL"
    ],
    "status": "Production",
    "images": [
      "projects/property-appraisal/arch.jpg",
      "projects/property-appraisal/sample.jpg",
      "projects/property-appraisal/ss.jpg"
    ],
    "video": "https://www.loom.com/embed/0c90f480866644fc9677b4d7feaeb4af",
    "videoPoster": "projects/property-appraisal/videoframe.png",
    "highlights": [
      "Automates 10,000+ appraisal PDFs/year; ~300 hours/month saved; faster customer turnaround.",
      "Dual‑pipeline architecture (OCR + LLM) with clean separation for maintainability.",
      "High‑accuracy extraction via preprocessing, layout parsing, and normalization.",
      "API orchestration (FastAPI) and dashboard (Streamlit) for review and export.",
      "JSONL artifact trail for traceability, debugging, and repeatable reprocessing.",
      "Production‑ready design with centralized logging/config and extensible NLP pipeline."
    ],
    "relatedProjects": [
      "etl-pipeline"
    ],
    "relatedPosts": [
      "fastapi-practices"
    ],
    "featured": true,
    "kind": "project"
  },
  {
    "id": "etl-pipeline",
    "title": "Automated ETL Pipeline",
    "role": "SDE Applied AI at Adaequare",
    "description": "Standardizes 50+ county tax roll datasets per year, reducing analyst time by 85%.",
    "contribution": "Designed intelligent ETL system that learns from each dataset...",
    "tech": [
      "Python",
      "Pandas",
      "Fuzzy Matching",
      "Semantic Patterns",
      "PostgreSQL",
      "Redis"
    ],
    "status": "Production",
    "images": [],
    "highlights": [
      "85% reduction in analyst time (8 hours → 1 hour)",
      "Self-learning: improves with each dataset processed"
    ],
    "featured": false,
    "kind": "project"
  },
  {
    "id": "trading-bot",
    "title": "Weekend Trading Bot",
    "description": "Scrappy bot built over a weekend to test strategies.",
    "tech": [
      "Python",
      "Pandas"
    ],
    "details": "Uses technical indicators and simple rules.",
    "highlights": [
      "Real-time data processing",
      "Backtesting framework"
    ],
    "images": [],
    "featured": false,
    "kind": "experimental"
  },
  {
    "id": "cli-tool",
    "title": "CLI Tool for Data Cleaning",
    "description": "Utility for quick CSV transformations.",
    "tech": [
      "Python",
      "Click"
    ],
    "details": "Handles repetitive CSV cleaning tasks.",
    "highlights": [
      "Quick transformations",
      "Batch processing"
    ],
    "images": [],
    "featured": false,
    "kind": "experimental"
  }
]