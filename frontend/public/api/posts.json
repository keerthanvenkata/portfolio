[
  {
    "id": "contract-intelligence-takeaways",
    "title": "Contract Intelligence Takeaways",
    "excerpt": "Lessons from building the MSA Metadata Extractor—document-as-knowledge-graph, semantic chunking, multi-LLM calls (extract + Judge), validation loops, and the Middle Ignorance problem.",
    "category": "Tech",
    "date": "2026-02-12",
    "author": "Keerthan Venkata",
    "featured": true,
    "content_html": "<p>While building the MSA Metadata Extractor—a contract intelligence system that extracts structured metadata from MSAs, NDAs, and service agreements—I ran into a problem that single-context extraction couldn't fix: long contracts. The same ideas show up in another project of mine, the <a href=\"https://github.com/keerthanvenkata/semantic-topic-mapper\">Semantic Topic Mapper</a>: treat the document as a knowledge graph—topic hierarchies, section boundaries, cross-references, entity relationships—and use that structure to drive chunking, multi-LLM calls, and validation loops. This post is about the experiments I ran, the lessons I took away, and how that graph-plus-pipeline architecture fits together.</p>\n<h2>The Challenge</h2>\n<p>Production today is a hybrid multimodal pipeline: we send PDFs (and sometimes DOCX) through a coordinator that treats text pages as text and image pages as images, then run Gemini Vision for extraction and template-based validation in a single flow. It works well for typical-length agreements and for &quot;Frankenstein&quot; docs (digital text plus scanned signature pages). The moment we pushed that design onto 50+ page contracts, we saw a pattern I started calling &quot;Middle Ignorance&quot;: the model would nail the preamble and the signature block but miss or invent things in the middle—e.g. liability caps in Section 14, termination clauses buried in amendments. We'd get high confidence scores on answers that were wrong. That's the failure mode that matters most in contract extraction: confident hallucinations, not &quot;I don't know.&quot;</p>\n<h2>Key Learnings</h2>\n<h3>1. Treat the Document as a Knowledge Graph (Then Chunk by It)</h3>\n<p>In the Semantic Topic Mapper I extract topic hierarchies, entity relationships, and cross-references from complex documents—effectively building a small knowledge graph over the text. That same mindset applies to contracts: before you run extraction, you need a notion of <em>structure</em>—sections, headings, references (e.g. &quot;as defined in Section 7&quot;). That structure is your graph: nodes are sections or topics, edges are &quot;contains&quot; or &quot;references&quot;. Once you have it (or approximate it with heuristics or a cheap first-pass model), you can chunk along section boundaries instead of arbitrary page breaks. So: <em>document structure as a knowledge graph</em> drives <em>semantic chunking</em>.</p>\n<h3>2. Semantic Chunking Beats Page-Based Splits</h3>\n<p>I tried feeding more of the document in one go (larger context, chunking by page count). Middle sections were still under-weighted. The lesson: split by semantic sections (e.g. &quot;Indemnification&quot;, &quot;Fees&quot;, &quot;Termination&quot;) so that each chunk is a coherent unit and the model isn't guessing where one idea ends and another begins. When you have a section/topic map (from the graph view above), chunking becomes deterministic: one chunk per section or per logical block, with provenance so the Judge knows where each candidate came from.</p>\n<h3>3. Candidates Plus an LLM Judge Cut Hallucinations (Multi-LLM Calls)</h3>\n<p>Keeping multiple candidates per field (one or more per chunk) and deferring the final choice was a big win. I added a second step—an LLM acting as a Judge—that takes the candidates and their surrounding text and selects or synthesizes the best answer with an explicit confidence score. Giving the Judge context around each candidate (not just the raw string) cut down hallucinations in my tests. So you get <em>multi-LLM calls</em> in a single job: one or more extraction calls per chunk, then a Judge call per field. The pipeline is deterministic (same chunks, same routing); the LLMs are the probability functions inside it.</p>\n<h3>4. Split Extraction and Validation—Validation Loops with a Cap</h3>\n<p>In production we had merged validation into the extraction prompt to save an API call. In experiments I split them: extraction first, then a validator that could reject and return structured feedback. The validator becomes a feedback signal for a <em>validation loop</em>: retry extraction or Judge with the feedback, then validate again. The critical constraint: max retries. After N attempts we stop and send to human review—no unbounded loops. So the pipeline is: chunk → extract (per chunk) → aggregate (Judge per field) → validate → if fail and retries left, retry with feedback; else escalate.</p>\n<h3>5. The Architecture I'm Betting On</h3>\n<p><strong>Graph-informed chunking + multi-LLM + validation loop.</strong> Use document structure (section/topic graph, or a good heuristic) to define chunks. Run extraction per chunk and collect candidates per field with provenance. Run an LLM Judge per field (candidates + context) to pick the best answer and a confidence score. Run a validation step that can reject and return structured feedback; on reject, retry (extraction or Judge) with that feedback, up to a hard cap, then send to human review. That's the full picture: knowledge-graph-style structure for chunking, semantic chunking for coverage, multi-LLM calls (extract + Judge) for consistency, and a deterministic validation loop so production stays safe.</p>\n<h2>Results</h2>\n<p>We didn't rip out the current system. It's still multimodal by default (Gemini Vision) because text-only models failed on scanned signature pages, and we kept hybrid extraction because it handles mixed PDFs better than a single mode. The refactor I'm describing is about how we run extraction and validation, not about replacing the existing pipeline. One non-negotiable for production: in high-stakes contract extraction, a false positive (confident wrong answer) is unacceptable. Evals must stress-test for hallucinations. Accuracy of what we output matters more than extracting every field.</p>\n<p>One-shot extraction over a giant context is the wrong default for long contracts. Treating the document as a knowledge graph (sections, topics, references) gives you semantic chunking; semantic chunking plus multi-LLM calls (extract per chunk, Judge per field) and a capped validation loop gave me the biggest gain in experiments. I'd make sure evals treat confident wrong as the failure mode you optimize against—and I'd keep the pipeline deterministic so the only probability is inside the model calls.</p>\n"
  },
  {
    "id": "etl-pipelines",
    "title": "Building Scalable ETL Pipelines",
    "excerpt": "Lessons learned from processing millions of records and designing self-learning systems.",
    "category": "Tech",
    "date": "2025-01-15",
    "author": "Keerthan Venkata",
    "featured": true,
    "content_html": "<p>Building ETL pipelines that scale isn't just about moving data from point A to point B. It's about creating systems that are resilient, maintainable, and can adapt to changing requirements.</p>\n<h2>The Challenge</h2>\n<p>At Adaequare, I faced the challenge of processing 50+ county tax roll datasets per year, each with over 20,000 records. The problem? Every dataset had different schemas, inconsistent column names, and varying data quality.</p>\n<h2>Key Learnings</h2>\n<h3>1. Design for Adaptability</h3>\n<p>Rather than hardcoding transformations, I built a system that learns from each dataset. Using fuzzy matching and semantic pattern recognition, the pipeline identifies similar fields across different schemas.</p>\n<h3>2. Feedback Loops Matter</h3>\n<p>We implemented a feedback mechanism where analyst corrections improve future processing. This reduced manual intervention by 85% over time.</p>\n<h3>3. Modular Architecture</h3>\n<p>Breaking the pipeline into distinct stages (extraction, transformation, validation, loading) made it easier to debug and optimize individual components.</p>\n<h2>Results</h2>\n<p>The system now handles datasets automatically that previously took analysts 8+ hours to process. It's scalable to 3,000+ US counties and gets smarter with each dataset.</p>\n<p>Building systems that learn and adapt is the future of ETL.</p>\n"
  },
  {
    "id": "aerospace-to-ai",
    "title": "From Aerospace to Applied AI",
    "excerpt": "How aerospace engineering—systems thinking, rigor, and problem decomposition—shaped my approach to software and applied AI.",
    "category": "Career",
    "date": "2025-01-08",
    "author": "Keerthan Venkata",
    "featured": false,
    "content_html": "<p>I studied aerospace engineering (B.Tech at IIT Madras, focused on autonomous systems and path planning) before moving into software and applied AI. The shift wasn’t a reset—the same habits that mattered in aerospace still drive how I design and build systems today.</p>\n<h2>Systems Thinking</h2>\n<p>Aerospace forces you to think in systems: sensors, actuators, control loops, and environment. You don’t optimize one part in isolation; you ask how a change here affects stability, latency, and failure modes elsewhere. I carry that into software: when I design an ETL pipeline or an LLM extraction system, I think about data flow, failure boundaries, and what happens when a component is slow or wrong. That’s why I care about validation steps, retry limits, and human review—they’re the control surfaces that keep the system safe when the “plant” (the model or the data) misbehaves.</p>\n<h2>Rigor and Decomposition</h2>\n<p>In aerospace you decompose a problem into clear stages (model, simulate, validate) and you’re explicit about assumptions and constraints. I do the same with ML and data pipelines: define the problem and the success criteria first, then design stages (extract, transform, validate, load) with clear inputs and outputs. That makes it easier to test, debug, and hand off. Rigor also means not trusting a single black box—we use evals, validation rules, and human review so we know when the system is wrong instead of assuming it’s right.</p>\n<h2>Transfer to Applied AI</h2>\n<p>Path planning and control theory are about feedback and robustness under uncertainty. In applied AI, uncertainty is everywhere: model outputs vary, inputs are messy, and requirements change. The same mindset applies: design for observability (logging, metrics, confidence scores), define failure modes (e.g. confident hallucinations in contracts), and build feedback loops (analyst corrections improving matching, validation guiding retries). My aerospace background didn’t teach me Python or transformer architectures, but it did teach me how to break hard problems into manageable pieces and how to keep systems reliable when the world doesn’t behave like the spec.</p>\n<p>If you’re moving from another engineering discipline into software or AI, the habits that made you good there—clear decomposition, explicit assumptions, and thinking in systems—translate more than you might expect.</p>\n"
  },
  {
    "id": "llm-patterns",
    "title": "LLM Integration Patterns",
    "excerpt": "Practical patterns for production LLM systems—fallback chains, cost control, prompt-as-code, and handling hallucinations.",
    "category": "Tech",
    "date": "2024-12-20",
    "author": "Keerthan Venkata",
    "featured": true,
    "content_html": "<p>Integrating LLMs into production is very different from prototyping in a notebook. After building contract extraction, event flyer extraction, and tax-roll matching systems, I've settled on a few patterns that keep systems reliable and costs predictable.</p>\n<p><strong>The mental model that helps most:</strong> working production systems today treat the LLM as a <em>probability function</em> inside a larger <em>deterministic</em> pipeline. The pipeline—chunking, routing, validation, retries, human escalation—is deterministic and testable. The LLM is the stochastic part: you constrain its inputs and outputs, you validate and retry, and you never let a single sample become the source of truth without checks. Once you design that way, fallback chains, caching, and prompt-as-code all slot in naturally.</p>\n<h2>Production Patterns That Actually Help</h2>\n<h3>1. Fallback Chains</h3>\n<p>Don't depend on a single model or provider. In our contract extractor we use Gemini as primary; if a call fails (rate limit, timeout, or bad response), we fall back to a lighter model or a cached result. The key is defining what &quot;failure&quot; means: we treat low confidence scores and validation failures as signals to retry or escalate, not just HTTP errors. Chain order matters: put the model that's best for your task first, and the one that's cheapest or fastest as the last resort so you only pay for fallbacks when you need them.</p>\n<h3>2. Cost Management: Caching and Batching</h3>\n<p>LLM calls are the main cost driver. We cache extraction results by a content hash (e.g. first page + structure hash) so identical or near-identical documents don't hit the API again. For batch jobs we group requests and use async calls with a concurrency limit so we don't burst rate limits. We also use smaller or &quot;flash&quot; models for simple fields and reserve the heavy model for complex sections—this cut our per-document cost significantly without hurting accuracy on the parts that matter.</p>\n<h3>3. Prompt Engineering as Code</h3>\n<p>Prompts live in versioned files or config, not inside application code. We use templates with placeholders (document chunk, field name, examples) and inject context at runtime. That makes it easy to A/B test prompt changes, roll back, and keep prompts consistent across environments. For validation we use structured output (JSON schema or Pydantic) so we can parse and validate in code instead of parsing free-form text.</p>\n<h2>Error Handling and Reliability</h2>\n<p>Production LLM systems fail in specific ways: hallucinations (confident wrong answers), inconsistent formatting, rate limits, and timeouts. We handle them explicitly.</p>\n<ul>\n<li><strong>Hallucinations</strong>: Validate against known rules or templates; when confidence is low or validation fails, send to human review instead of auto-publishing. In contract extraction we never treat a single model response as final for high-stakes fields without a Judge or validation step.</li>\n<li><strong>Rate limits and timeouts</strong>: Exponential backoff, per-provider limits, and a hard cap on retries so we don't spin forever. After N failures we fail the job and alert.</li>\n<li><strong>Monitoring</strong>: Log token usage, latency, and validation pass/fail rates. Track cost per document or per job so you can see when a new prompt or model changes the bill.</li>\n</ul>\n<h2>Summary</h2>\n<p>Treat the LLM as a probability function inside a deterministic pipeline: the pipeline is the system; the model is one step. Use fallback chains for availability, cache and batch for cost, and keep prompts in code/config. Design for hallucinations and rate limits from day one, and make validation and human review first-class so production stays safe and predictable.</p>\n"
  },
  {
    "id": "startup-journey",
    "title": "Building a Startup While Working",
    "excerpt": "Reflections on co-founding QFI Capital while working full-time—burnout, passion, time management, staying grounded, and when to ship vs when to invest in quality.",
    "category": "Entrepreneurship",
    "date": "2024-12-10",
    "author": "Keerthan Venkata",
    "featured": true,
    "content_html": "<p>Co-founding QFI Research Capital while working full-time at Adaequare taught me more about entrepreneurship than any book or course. It was messy, exhausting, and one of the most formative stretches of my career. Here’s what stuck.</p>\n<h2>The Reality Check</h2>\n<p>It’s late nights after your day job, weekends spent on architecture and debugging, and constant context switching between two demanding roles. There were days I burnt out—no way around it when you’re carrying two full-time loads. What got me through: building something I was genuinely passionate about, so the hard days felt meaningful, and managing time properly so I didn’t run myself into the ground. Your employer deserves your best during work hours; the startup gets the edges. That only works if you’re honest with both sides and if you protect time for deep work instead of filling every gap with meetings.</p>\n<h2>What I Learned</h2>\n<h3>Time Management is Critical</h3>\n<p>I had to be ruthless about when and where I did what. Mornings (before work) were for system design and architecture—when my head was clearest. Evenings were for implementation and code review. Weekends were for the heavy lifting: integration, testing, and the kind of focus that doesn’t survive a 9–5 interrupt. Blocking time and saying no to non-essentials was the only way to make progress and avoid burning out. Manage time properly; otherwise the passion isn’t enough.</p>\n<h3>Choose Your Co-founders Wisely</h3>\n<p>We divided responsibilities clearly: who owned trading logic, who owned infra and data, who talked to early users. Trust was non-negotiable—we couldn’t second-guess each other when we were already stretched thin. Regular syncs (even short ones) kept us aligned so small misalignments didn’t become big ones.</p>\n<h3>Technical Debt vs Speed</h3>\n<p>Done is better than perfect when you’re validating an idea, but “done” doesn’t mean unmaintainable. We shipped an MVP with a clear path to refactor: event-driven core, clear boundaries between services, and observability from day one. When we hit scale or stability issues, we had a codebase we could fix instead of one we had to rewrite. The lesson: know when to move fast and when to invest in quality. If the next milestone depends on reliability or scale, slow down and build it right.</p>\n<h3>Stay Grounded</h3>\n<p>I second-guessed myself more than once—whether I could actually handle the responsibility, the deadlines, the weight of building something real. The antidote wasn’t fake confidence; it was staying grounded. Moonshot dreams are fine; the work and the attitude have to be grounded. Show up, do the next concrete thing, and don’t let the big vision turn into anxiety. Grounded work, day after day, is what makes the moonshot possible.</p>\n<h2>The Platform</h2>\n<p>QFI was (and is) a mid-frequency algorithmic trading platform: microservices, Kafka for market data and events, PostgreSQL and InfluxDB for orders and time-series, and real-time risk and analytics. Building that while employed meant every component had to earn its place—no “nice to have” features until the core was solid.</p>\n<h2>Key Takeaway</h2>\n<p>Entrepreneurship while employed is possible, but it requires sacrifice, discipline, and a clear agreement with yourself and your co-founders about what you’re optimizing for. Build something you’re passionate about so the hard days are worth it; manage time properly so you don’t burn out. Keep the dreams big and the work grounded. For me it was worth it: the platform went live, we learned what we couldn’t have learned from a side project alone, and I carried those lessons into every role since.</p>\n"
  },
  {
    "id": "fastapi-practices",
    "title": "FastAPI Best Practices",
    "excerpt": "Dependency injection, Pydantic, async I/O, and deployment patterns for production-ready FastAPI apps.",
    "category": "Tutorial",
    "date": "2024-11-15",
    "author": "Keerthan Venkata",
    "featured": false,
    "content_html": "<p>FastAPI makes it easy to ship an API quickly, but production readiness comes from a few deliberate choices. Here’s what I use on real projects: dependency injection, Pydantic everywhere, background tasks, and clear error handling.</p>\n<h2>Dependency Injection</h2>\n<p>Use FastAPI’s <code>Depends()</code> for everything that can be configured or swapped: DB sessions, config, clients (e.g. LLM or storage). Define a function that returns the dependency and inject it into route handlers. That gives you a single place to control lifecycle (e.g. one session per request) and makes testing trivial—you override the dependency in the test client instead of patching globals.</p>\n<pre><code class=\"language-python\">def get_db():\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n\n@router.post(&quot;/items&quot;)\ndef create_item(item: ItemCreate, db: Session = Depends(get_db)):\n    ...\n</code></pre>\n<p>Keep dependencies thin: they should create or fetch an object, not contain business logic.</p>\n<h2>Pydantic for Request and Response</h2>\n<p>Define Pydantic models for every request body and response. Use <code>response_model</code> (or <code>response_model_exclude_none</code>) so the API contract is explicit and docs stay accurate. For validation, use custom validators and <code>Field()</code> for constraints and descriptions. That way invalid input is rejected at the edge and your handlers work with typed objects, not raw dicts. For large or nested responses, consider <code>model_config = ConfigDict(from_attributes=True)</code> and returning ORM models only where you need to avoid N+1; otherwise prefer explicit response models.</p>\n<h2>Background Tasks and Async</h2>\n<p>For work that doesn’t need to block the response (emails, post-processing, enqueueing jobs), use <code>BackgroundTasks</code>. Add the task in the route and return immediately; FastAPI runs it after the response is sent. For I/O-bound work (DB, HTTP calls to LLMs or external APIs), use <code>async</code> route handlers and async libraries (e.g. <code>asyncpg</code>, <code>httpx</code>) so a single process can handle many concurrent requests without blocking. Don’t mix blocking calls in async routes—offload those to a thread pool or a worker.</p>\n<h2>Exception Handlers</h2>\n<p>Register custom exception handlers with <code>@app.exception_handler()</code>. Map domain exceptions (e.g. <code>ValidationError</code>, <code>NotFound</code>) to HTTP status codes and a consistent JSON shape (e.g. <code>{&quot;detail&quot;: &quot;...&quot;, &quot;code&quot;: &quot;...&quot;}</code>). That way clients get predictable errors and you can log and monitor by exception type. For validation errors from Pydantic, FastAPI’s default 422 is fine; for business rules, use 4xx and reserve 5xx for unexpected failures.</p>\n<h2>Testing and Deployment</h2>\n<ul>\n<li><strong>Tests</strong>: Use <code>TestClient</code> and override dependencies (e.g. in-memory DB or mock LLM client) so routes are tested in isolation. Test success and failure paths and that response models match what you expect.</li>\n<li><strong>Deployment</strong>: Run behind a reverse proxy (e.g. Nginx or a cloud load balancer). Use workers (e.g. Gunicorn with uvicorn workers) and set worker count from CPU and I/O characteristics. Pull config from environment variables or a secret manager; never hardcode secrets. Health checks (<code>/health</code> or <code>/ready</code>) should verify DB and critical dependencies so the orchestrator can restart unhealthy instances.</li>\n</ul>\n<p>Adopting these patterns keeps FastAPI apps maintainable, testable, and ready for production traffic.</p>\n"
  }
]