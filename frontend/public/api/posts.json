[
  {
    "id": "contract-intelligence-takeaways",
    "title": "Contract Intelligence Takeaways",
    "excerpt": "Lessons from building the MSA Metadata Extractor—experiments with long-context LLMs, the Middle Ignorance problem, and a chunk-then-aggregate architecture with an LLM Judge.",
    "category": "Tech",
    "date": "2026-02-12",
    "author": "Keerthan Venkata",
    "featured": true,
    "content_html": "<p>While building the MSA Metadata Extractor—a contract intelligence system that extracts structured metadata from MSAs, NDAs, and service agreements—I ran into a problem that single-context extraction couldn't fix: long contracts. This post is about the experiments I ran, the lessons I took away, and the architecture I'm convinced is the right next step.</p>\n<h2>The Challenge</h2>\n<p>Production today is a hybrid multimodal pipeline: we send PDFs (and sometimes DOCX) through a coordinator that treats text pages as text and image pages as images, then run Gemini Vision for extraction and template-based validation in a single flow. It works well for typical-length agreements and for &quot;Frankenstein&quot; docs (digital text plus scanned signature pages). The moment we pushed that design onto 50+ page contracts, we saw a pattern I started calling &quot;Middle Ignorance&quot;: the model would nail the preamble and the signature block but miss or invent things in the middle—e.g. liability caps in Section 14, termination clauses buried in amendments. We'd get high confidence scores on answers that were wrong. That's the failure mode that matters most in contract extraction: confident hallucinations, not &quot;I don't know.&quot;</p>\n<h2>Key Learnings</h2>\n<h3>1. Semantic Chunking Beats Page-Based Splits</h3>\n<p>I tried feeding more of the document in one go (larger context, chunking by page count). Middle sections were still under-weighted. The lesson: split by semantic sections (e.g. &quot;Indemnification&quot;, &quot;Fees&quot;, &quot;Termination&quot;) so that each chunk is a coherent unit and the model isn't guessing where one idea ends and another begins.</p>\n<h3>2. Candidates Plus an LLM Judge Cut Hallucinations</h3>\n<p>Keeping multiple candidates per field (one or more per chunk) and deferring the final choice was a big win. I added a second step—an LLM acting as a Judge—that takes the candidates and their surrounding text and selects or synthesizes the best answer with an explicit confidence score. Giving the Judge context around each candidate (not just the raw string) cut down hallucinations in my tests.</p>\n<h3>3. Split Extraction and Validation</h3>\n<p>In production we had merged validation into the extraction prompt to save an API call. In experiments I split them: extraction first, then a validator that could reject and return structured feedback. The validator becomes a feedback signal for an agent loop. The critical constraint: max retries. After N attempts we stop and send to human review—no unbounded loops.</p>\n<h3>4. The Architecture I'm Betting On</h3>\n<p>Chunk by section boundaries, run extraction per chunk and collect candidates per field with provenance, then run an LLM Judge per field (candidates + context) to pick the best answer and output a confidence score. Add a validation step that can reject and request a retry with feedback, with a hard cap on retries. That's the semantic chunking + aggregation design: chunk for coverage, aggregate with a Judge for consistency, and use validation as a control loop.</p>\n<h2>Results</h2>\n<p>We didn't rip out the current system. It's still multimodal by default (Gemini Vision) because text-only models failed on scanned signature pages, and we kept hybrid extraction because it handles mixed PDFs better than a single mode. The refactor I'm describing is about how we run extraction and validation, not about replacing the existing pipeline. One non-negotiable for production: in high-stakes contract extraction, a false positive (confident wrong answer) is unacceptable. Evals must stress-test for hallucinations. Accuracy of what we output matters more than extracting every field.</p>\n<p>One-shot extraction over a giant context is the wrong default for long contracts. Semantic chunking plus candidate generation and an LLM Judge that sees context gave me the biggest gain in experiments—and I'd make sure evals treat confident wrong as the failure mode you optimize against.</p>\n"
  },
  {
    "id": "etl-pipelines",
    "title": "Building Scalable ETL Pipelines",
    "excerpt": "Lessons learned from processing millions of records and designing self-learning systems.",
    "category": "Tech",
    "date": "2025-01-15",
    "author": "Keerthan Venkata",
    "featured": true,
    "content_html": "<p>Building ETL pipelines that scale isn't just about moving data from point A to point B. It's about creating systems that are resilient, maintainable, and can adapt to changing requirements.</p>\n<h2>The Challenge</h2>\n<p>At Adaequare, I faced the challenge of processing 50+ county tax roll datasets per year, each with over 20,000 records. The problem? Every dataset had different schemas, inconsistent column names, and varying data quality.</p>\n<h2>Key Learnings</h2>\n<h3>1. Design for Adaptability</h3>\n<p>Rather than hardcoding transformations, I built a system that learns from each dataset. Using fuzzy matching and semantic pattern recognition, the pipeline identifies similar fields across different schemas.</p>\n<h3>2. Feedback Loops Matter</h3>\n<p>We implemented a feedback mechanism where analyst corrections improve future processing. This reduced manual intervention by 85% over time.</p>\n<h3>3. Modular Architecture</h3>\n<p>Breaking the pipeline into distinct stages (extraction, transformation, validation, loading) made it easier to debug and optimize individual components.</p>\n<h2>Results</h2>\n<p>The system now handles datasets automatically that previously took analysts 8+ hours to process. It's scalable to 3,000+ US counties and gets smarter with each dataset.</p>\n<p>Building systems that learn and adapt is the future of ETL.</p>\n"
  },
  {
    "id": "aerospace-to-ai",
    "title": "From Aerospace to Applied AI",
    "excerpt": "How aerospace engineering shaped my software approach.",
    "category": "Career",
    "date": "2025-01-08",
    "author": "Keerthan Venkata",
    "featured": false,
    "content_html": "<p>Systems thinking, mathematical rigor, and problem decomposition transfer directly from aerospace to software architecture and AI/ML.</p>\n"
  },
  {
    "id": "llm-patterns",
    "title": "LLM Integration Patterns",
    "excerpt": "Practical patterns for production LLM systems and cost control.",
    "category": "Tech",
    "date": "2024-12-20",
    "author": "Keerthan Venkata",
    "featured": true,
    "content_html": "<p>Integrating LLMs into production systems is very different from experimenting in a notebook. Here are patterns I've learned building real systems.</p>\n<h2>Production Patterns</h2>\n<ol>\n<li>Fallback chains</li>\n<li>Cost management via caching and batching</li>\n<li>Prompt engineering as code</li>\n</ol>\n<h2>Error Handling</h2>\n<p>Expect hallucinations, inconsistent output, rate limits, and timeouts. Build robust monitoring and retries.</p>\n"
  },
  {
    "id": "startup-journey",
    "title": "Building a Startup While Working",
    "excerpt": "Reflections on co-founding QFI Capital while working full-time.",
    "category": "Entrepreneurship",
    "date": "2024-12-10",
    "author": "Keerthan Venkata",
    "featured": true,
    "content_html": "<p>Co-founding QFI Research Capital while working full-time at Adaequare taught me more about entrepreneurship than any book or course ever could.</p>\n<h2>The Reality Check</h2>\n<p>It's late nights after your day job, weekends spent debugging, and constant context switching between two demanding roles.</p>\n<h2>What I Learned</h2>\n<h3>Time Management is Critical</h3>\n<p>Ruthless prioritization. Mornings for architecture and system design, evenings for implementation, weekends for deep work.</p>\n<h3>Choose Your Co-founders Wisely</h3>\n<p>Divide responsibilities clearly and trust each other to execute.</p>\n<h3>Technical Debt vs Speed</h3>\n<p>Done is better than perfect, but know when to invest in quality.</p>\n<h2>The Platform</h2>\n<p>Microservices with Kafka, PostgreSQL + InfluxDB, and real-time risk engines.</p>\n<h2>Key Takeaway</h2>\n<p>Entrepreneurship while employed is possible, but it requires sacrifice, discipline, and a burning desire to build something meaningful.</p>\n"
  },
  {
    "id": "fastapi-practices",
    "title": "FastAPI Best Practices",
    "excerpt": "Tips and patterns for production-ready FastAPI.",
    "category": "Tutorial",
    "date": "2024-11-15",
    "author": "Keerthan Venkata",
    "featured": false,
    "content_html": "<p>Use dependency injection, Pydantic models, background tasks, and custom exception handlers. Prioritize testing, async I/O, and proper deployment.</p>\n"
  }
]