[
  {
    "id": "contract-intelligence-takeaways",
    "title": "Contract Intelligence Takeaways",
    "excerpt": "What I tried, what broke, and what I'd do next: experiments with long-context LLMs, the Middle Ignorance problem, and designing a chunk-then-aggregate architecture with an LLM Judge.",
    "category": "Tech",
    "date": "2026-02-12",
    "author": "Keerthan Venkata",
    "featured": true,
    "content_html": "<p>While building the <strong>MSA Metadata Extractor</strong> (a contract intelligence system that pulls structured metadata from MSAs, NDAs, and service agreements), I ran into a problem that single-context extraction couldn't fix: <strong>long contracts</strong>. This post is about the experiments I ran, the lessons I took away, and the architecture I'm convinced is the right next step — <strong>semantic chunking plus aggregation</strong>, with an <strong>LLM as Judge</strong>.</p>\n<hr>\n<h2>What I Built First (and Where It Breaks)</h2>\n<p>Production today is a <strong>hybrid multimodal</strong> pipeline: we send PDFs (and sometimes DOCX) through a coordinator that treats text pages as text and image pages as images, then run <strong>Gemini Vision</strong> for extraction and <strong>template-based validation</strong> in a single flow. It works well for typical-length agreements and for &quot;Frankenstein&quot; docs (digital text plus scanned signature pages). The trade-off we accepted: <strong>one big context</strong> per document (or per section) and validation baked into the same prompt to save latency.</p>\n<p>The moment we pushed that design onto <strong>50+ page contracts</strong>, we saw a pattern I started calling <strong>&quot;Middle Ignorance&quot;</strong>: the model would nail the preamble and the signature block but <strong>miss or invent</strong> things in the middle — e.g. liability caps in Section 14, termination clauses buried in amendments. We'd get high confidence scores on answers that were <strong>wrong</strong>. That's the failure mode that matters most in contract extraction: <strong>confident hallucinations</strong>, not &quot;I don't know.&quot;</p>\n<p>So I started experimenting with ways to reduce that risk without throwing away what already worked.</p>\n<hr>\n<h2>Experiment 1: Bigger Context vs. Smarter Chunking</h2>\n<p><strong>What I tried:</strong> First I tried feeding more of the document in one go (larger context, better chunking by page count). Result: <strong>middle sections were still under-weighted</strong>. The model behaved as if it was attending more to the start and end of the context; the middle was where errors and hallucinations showed up.</p>\n<p><strong>Lesson:</strong> Naive &quot;more context&quot; or &quot;split by N pages&quot; doesn't fix the problem. You need the model to <strong>see the middle as first-class</strong>. That pushed me toward <strong>semantic boundaries</strong> instead of arbitrary ones: split by <strong>sections</strong> (e.g. &quot;Indemnification&quot;, &quot;Fees&quot;, &quot;Termination&quot;) so that each chunk is a coherent unit and the model isn't guessing where one idea ends and another begins.</p>\n<hr>\n<h2>Experiment 2: One Shot vs. Many Candidates</h2>\n<p><strong>What I tried:</strong> I ran extractions <strong>per semantic chunk</strong> and compared a single &quot;best&quot; answer per field vs. <strong>keeping multiple candidates</strong> (one or more per chunk) and deferring the final choice.</p>\n<p><strong>Lesson:</strong> Keeping <strong>candidates</strong> and choosing later was a big win. When we had only one answer per field from one big call, we had no way to reconcile conflicting signals. When we had <strong>candidates with provenance</strong> (which chunk they came from), we could:</p>\n<ul>\n<li>See when the model was contradicting itself across chunks.</li>\n<li>Feed those candidates plus <strong>context</strong> into a second step to decide the final value.</li>\n</ul>\n<p>That second step is what I started calling the <strong>LLM-as-Judge</strong>: a separate call that takes the candidates and their surrounding text and <strong>selects or synthesizes</strong> the best answer, with an explicit <strong>confidence score</strong>. Giving the Judge <strong>context around each candidate</strong> (not just the raw string) cut down hallucinations a lot in my tests — the Judge could reject &quot;this looks like it came from the wrong section.&quot;</p>\n<hr>\n<h2>Experiment 3: Extraction and Validation in One Call vs. Separate</h2>\n<p><strong>What I tried:</strong> In production we had merged extraction and validation into one prompt (ADR-0003) to save an API call and latency. In experiments I <strong>split</strong> them: extraction first, then a validator that could <strong>reject</strong> and return <strong>structured feedback</strong> (e.g. &quot;Termination Date is missing&quot;, &quot;Liability Cap doesn't match template&quot;).</p>\n<p><strong>Lesson:</strong> <strong>Splitting extraction and validation</strong> is worth the extra call. The validator becomes a <strong>feedback signal</strong> for an agent loop: &quot;retry with this guidance&quot; instead of &quot;accept or flag.&quot; The critical constraint: <strong>max retries</strong>. I never let the loop run unbounded; after N attempts we <strong>stop and send to human review</strong>. That keeps the system predictable and avoids silent infinite retries.</p>\n<hr>\n<h2>The Architecture I'm Betting On: Chunk → Candidates → Judge → (Optional) Agent Loop</h2>\n<p>Putting it together, the architecture I'd implement next is:</p>\n<ol>\n<li>\n<p><strong>Semantic chunking</strong> — Split the document by <strong>section boundaries</strong> (headings, structure), not by page count. Each chunk is a meaningful unit (e.g. one clause or one section).</p>\n</li>\n<li>\n<p><strong>Candidate generation</strong> — Run extraction <strong>per chunk</strong> (or per group of chunks) and collect <strong>candidates per field</strong> with provenance (chunk id, snippet).</p>\n</li>\n<li>\n<p><strong>LLM Judge</strong> — One dedicated call per field (or per group of fields) that takes <strong>candidates + context</strong> and:</p>\n<ul>\n<li>Picks the best candidate or synthesizes from several.</li>\n<li>Outputs a <strong>confidence score</strong> and short reasoning.</li>\n</ul>\n</li>\n<li>\n<p><strong>Validation as feedback (agentic loop)</strong> — A separate validation step that can <strong>reject</strong> the Judge's output and ask for a retry with specific feedback; <strong>MAX_RETRIES</strong> then hand off to humans.</p>\n</li>\n</ol>\n<p>This is the <strong>semantic chunking + aggregation</strong> design: chunk for coverage, aggregate with a Judge for consistency and lower hallucinations, and use validation as a control loop rather than a one-way gate.</p>\n<hr>\n<h2>Innovations and Non-Negotiables</h2>\n<p><strong>Innovations I care about most:</strong></p>\n<ul>\n<li><strong>Semantic chunking</strong> as the unit of work, not pages or tokens.</li>\n<li><strong>Explicit candidate list</strong> with provenance so the Judge (and humans) can see where answers came from.</li>\n<li><strong>Judge with context</strong> — the Judge sees the candidate <em>and</em> the surrounding text, not just the extracted string.</li>\n<li><strong>Validation as feedback</strong> with a <strong>hard cap on retries</strong> so we never loop forever.</li>\n</ul>\n<p><strong>Non-negotiable for production:</strong> In high-stakes contract extraction, a <strong>false positive</strong> (confident wrong answer) is unacceptable. A <strong>false negative</strong> (low confidence, flag for review) is acceptable. So evals and QC have to <strong>stress-test for hallucinations</strong>: e.g. &quot;model confidently extracted a liability cap that doesn't exist&quot; must count as a failure. <strong>Accuracy of what we output</strong> matters more than extracting every field.</p>\n<hr>\n<h2>What's Still in Production (and Why)</h2>\n<p>We didn't rip out the current system. It's still <strong>multimodal by default</strong> (Gemini Vision) because text-only models failed on scanned signature pages; we accepted the cost for reliability on signatories. We kept <strong>hybrid extraction</strong> (text vs. image pages) because it handles mixed PDFs better than a single mode. The refactor I'm describing is about <strong>how we run extraction and validation</strong> (chunk → candidates → Judge → validation loop), not about replacing the existing PDF/image pipeline. The codebase is modular — you can swap the LLM client or the chunking strategy without breaking the API.</p>\n<hr>\n<h2>Summary</h2>\n<p>My main takeaway from the MSA extractor work: <strong>one-shot extraction over a giant context is the wrong default for long contracts.</strong> Semantic chunking plus <strong>candidate generation</strong> and an <strong>LLM Judge</strong> that sees context gave me the biggest gain in experiments. Splitting extraction and validation and adding a <strong>bounded agent loop</strong> is the next step I'd take. If you're building something similar, I'd start there — and I'd make sure your evals treat <strong>confident wrong</strong> as the failure mode you optimize against.</p>\n"
  },
  {
    "id": "etl-pipelines",
    "title": "Building Scalable ETL Pipelines",
    "excerpt": "Lessons learned from processing millions of records and designing self-learning systems.",
    "category": "Tech",
    "date": "2025-01-15",
    "author": "Keerthan Venkata",
    "featured": true,
    "content_html": "<p>Building ETL pipelines that scale isn't just about moving data from point A to point B. It's about creating systems that are resilient, maintainable, and can adapt to changing requirements.</p>\n<h2>The Challenge</h2>\n<p>At Adaequare, I faced the challenge of processing 50+ county tax roll datasets per year, each with over 20,000 records. The problem? Every dataset had different schemas, inconsistent column names, and varying data quality.</p>\n<h2>Key Learnings</h2>\n<h3>1. Design for Adaptability</h3>\n<p>Rather than hardcoding transformations, I built a system that learns from each dataset. Using fuzzy matching and semantic pattern recognition, the pipeline identifies similar fields across different schemas.</p>\n<h3>2. Feedback Loops Matter</h3>\n<p>We implemented a feedback mechanism where analyst corrections improve future processing. This reduced manual intervention by 85% over time.</p>\n<h3>3. Modular Architecture</h3>\n<p>Breaking the pipeline into distinct stages (extraction, transformation, validation, loading) made it easier to debug and optimize individual components.</p>\n<h2>Results</h2>\n<p>The system now handles datasets automatically that previously took analysts 8+ hours to process. It's scalable to 3,000+ US counties and gets smarter with each dataset.</p>\n<p>Building systems that learn and adapt is the future of ETL.</p>\n"
  },
  {
    "id": "aerospace-to-ai",
    "title": "From Aerospace to Applied AI",
    "excerpt": "How aerospace engineering shaped my software approach.",
    "category": "Career",
    "date": "2025-01-08",
    "author": "Keerthan Venkata",
    "featured": false,
    "content_html": "<p>Systems thinking, mathematical rigor, and problem decomposition transfer directly from aerospace to software architecture and AI/ML.</p>\n"
  },
  {
    "id": "llm-patterns",
    "title": "LLM Integration Patterns",
    "excerpt": "Practical patterns for production LLM systems and cost control.",
    "category": "Tech",
    "date": "2024-12-20",
    "author": "Keerthan Venkata",
    "featured": true,
    "content_html": "<p>Integrating LLMs into production systems is very different from experimenting in a notebook. Here are patterns I've learned building real systems.</p>\n<h2>Production Patterns</h2>\n<ol>\n<li>Fallback chains</li>\n<li>Cost management via caching and batching</li>\n<li>Prompt engineering as code</li>\n</ol>\n<h2>Error Handling</h2>\n<p>Expect hallucinations, inconsistent output, rate limits, and timeouts. Build robust monitoring and retries.</p>\n"
  },
  {
    "id": "startup-journey",
    "title": "Building a Startup While Working",
    "excerpt": "Reflections on co-founding QFI Capital while working full-time.",
    "category": "Entrepreneurship",
    "date": "2024-12-10",
    "author": "Keerthan Venkata",
    "featured": true,
    "content_html": "<p>Co-founding QFI Research Capital while working full-time at Adaequare taught me more about entrepreneurship than any book or course ever could.</p>\n<h2>The Reality Check</h2>\n<p>It's late nights after your day job, weekends spent debugging, and constant context switching between two demanding roles.</p>\n<h2>What I Learned</h2>\n<h3>Time Management is Critical</h3>\n<p>Ruthless prioritization. Mornings for architecture and system design, evenings for implementation, weekends for deep work.</p>\n<h3>Choose Your Co-founders Wisely</h3>\n<p>Divide responsibilities clearly and trust each other to execute.</p>\n<h3>Technical Debt vs Speed</h3>\n<p>Done is better than perfect, but know when to invest in quality.</p>\n<h2>The Platform</h2>\n<p>Microservices with Kafka, PostgreSQL + InfluxDB, and real-time risk engines.</p>\n<h2>Key Takeaway</h2>\n<p>Entrepreneurship while employed is possible, but it requires sacrifice, discipline, and a burning desire to build something meaningful.</p>\n"
  },
  {
    "id": "fastapi-practices",
    "title": "FastAPI Best Practices",
    "excerpt": "Tips and patterns for production-ready FastAPI.",
    "category": "Tutorial",
    "date": "2024-11-15",
    "author": "Keerthan Venkata",
    "featured": false,
    "content_html": "<p>Use dependency injection, Pydantic models, background tasks, and custom exception handlers. Prioritize testing, async I/O, and proper deployment.</p>\n"
  }
]