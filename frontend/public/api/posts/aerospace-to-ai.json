{
  "id": "aerospace-to-ai",
  "title": "From Aerospace to Applied AI",
  "excerpt": "How aerospace engineering—systems thinking, rigor, and problem decomposition—shaped my approach to software and applied AI.",
  "category": "Career",
  "date": "2025-01-08",
  "author": "Keerthan Venkata",
  "featured": false,
  "content_html": "<p>I studied aerospace engineering (B.Tech at IIT Madras, focused on autonomous systems and path planning) before moving into software and applied AI. The shift wasn’t a reset—the same habits that mattered in aerospace still drive how I design and build systems today.</p>\n<h2>Systems Thinking</h2>\n<p>Aerospace forces you to think in systems: sensors, actuators, control loops, and environment. You don’t optimize one part in isolation; you ask how a change here affects stability, latency, and failure modes elsewhere. I carry that into software: when I design an ETL pipeline or an LLM extraction system, I think about data flow, failure boundaries, and what happens when a component is slow or wrong. That’s why I care about validation steps, retry limits, and human review—they’re the control surfaces that keep the system safe when the “plant” (the model or the data) misbehaves.</p>\n<h2>Rigor and Decomposition</h2>\n<p>In aerospace you decompose a problem into clear stages (model, simulate, validate) and you’re explicit about assumptions and constraints. I do the same with ML and data pipelines: define the problem and the success criteria first, then design stages (extract, transform, validate, load) with clear inputs and outputs. That makes it easier to test, debug, and hand off. Rigor also means not trusting a single black box—we use evals, validation rules, and human review so we know when the system is wrong instead of assuming it’s right.</p>\n<h2>Transfer to Applied AI</h2>\n<p>Path planning and control theory are about feedback and robustness under uncertainty. In applied AI, uncertainty is everywhere: model outputs vary, inputs are messy, and requirements change. The same mindset applies: design for observability (logging, metrics, confidence scores), define failure modes (e.g. confident hallucinations in contracts), and build feedback loops (analyst corrections improving matching, validation guiding retries). My aerospace background didn’t teach me Python or transformer architectures, but it did teach me how to break hard problems into manageable pieces and how to keep systems reliable when the world doesn’t behave like the spec.</p>\n<p>If you’re moving from another engineering discipline into software or AI, the habits that made you good there—clear decomposition, explicit assumptions, and thinking in systems—translate more than you might expect.</p>\n"
}