{
  "id": "llm-patterns",
  "title": "LLM Integration Patterns",
  "excerpt": "Practical patterns for production LLM systems—fallback chains, cost control, prompt-as-code, and handling hallucinations.",
  "category": "Tech",
  "date": "2024-12-20",
  "author": "Keerthan Venkata",
  "featured": true,
  "content_html": "<p>Integrating LLMs into production is very different from prototyping in a notebook. After building contract extraction, event flyer extraction, and tax-roll matching systems, I've settled on a few patterns that keep systems reliable and costs predictable.</p>\n<p><strong>The mental model that helps most:</strong> working production systems today treat the LLM as a <em>probability function</em> inside a larger <em>deterministic</em> pipeline. The pipeline—chunking, routing, validation, retries, human escalation—is deterministic and testable. The LLM is the stochastic part: you constrain its inputs and outputs, you validate and retry, and you never let a single sample become the source of truth without checks. Once you design that way, fallback chains, caching, and prompt-as-code all slot in naturally.</p>\n<h2>Production Patterns That Actually Help</h2>\n<h3>1. Fallback Chains</h3>\n<p>Don't depend on a single model or provider. In our contract extractor we use Gemini as primary; if a call fails (rate limit, timeout, or bad response), we fall back to a lighter model or a cached result. The key is defining what &quot;failure&quot; means: we treat low confidence scores and validation failures as signals to retry or escalate, not just HTTP errors. Chain order matters: put the model that's best for your task first, and the one that's cheapest or fastest as the last resort so you only pay for fallbacks when you need them.</p>\n<h3>2. Cost Management: Caching and Batching</h3>\n<p>LLM calls are the main cost driver. We cache extraction results by a content hash (e.g. first page + structure hash) so identical or near-identical documents don't hit the API again. For batch jobs we group requests and use async calls with a concurrency limit so we don't burst rate limits. We also use smaller or &quot;flash&quot; models for simple fields and reserve the heavy model for complex sections—this cut our per-document cost significantly without hurting accuracy on the parts that matter.</p>\n<h3>3. Prompt Engineering as Code</h3>\n<p>Prompts live in versioned files or config, not inside application code. We use templates with placeholders (document chunk, field name, examples) and inject context at runtime. That makes it easy to A/B test prompt changes, roll back, and keep prompts consistent across environments. For validation we use structured output (JSON schema or Pydantic) so we can parse and validate in code instead of parsing free-form text.</p>\n<h2>Error Handling and Reliability</h2>\n<p>Production LLM systems fail in specific ways: hallucinations (confident wrong answers), inconsistent formatting, rate limits, and timeouts. We handle them explicitly.</p>\n<ul>\n<li><strong>Hallucinations</strong>: Validate against known rules or templates; when confidence is low or validation fails, send to human review instead of auto-publishing. In contract extraction we never treat a single model response as final for high-stakes fields without a Judge or validation step.</li>\n<li><strong>Rate limits and timeouts</strong>: Exponential backoff, per-provider limits, and a hard cap on retries so we don't spin forever. After N failures we fail the job and alert.</li>\n<li><strong>Monitoring</strong>: Log token usage, latency, and validation pass/fail rates. Track cost per document or per job so you can see when a new prompt or model changes the bill.</li>\n</ul>\n<h2>Summary</h2>\n<p>Treat the LLM as a probability function inside a deterministic pipeline: the pipeline is the system; the model is one step. Use fallback chains for availability, cache and batch for cost, and keep prompts in code/config. Design for hallucinations and rate limits from day one, and make validation and human review first-class so production stays safe and predictable.</p>\n"
}